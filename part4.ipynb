{
 "cells": [
  {
   "cell_type": "markdown",
   "id": "fc90f1b3",
   "metadata": {},
   "source": [
    "# Project work, part 4 - Machine Learning"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "5820bffd",
   "metadata": {},
   "source": [
    "## Project Links\n",
    "\n",
    "**Github Repository:** https://github.com/fayomitz/IND320-fayomitz\n",
    "\n",
    "**Streamlit app:** https://ind320-fayomitz.streamlit.app/"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "fa69dce6",
   "metadata": {},
   "source": [
    "## AI Usage\n",
    "\n",
    "Throughout this project, AI assistance (GitHub Copilot) was utilized in several key areas:\n",
    "\n",
    "- **GeoJSON Integration**: Assistance with loading and processing Norwegian GeoJSON files for price areas and municipalities, implementing Shapely for point-in-polygon detection, and mapping municipalities to price areas\n",
    "- **Folium Map Development**: Help with implementing dynamic layer switching based on zoom level (price areas vs municipalities), choropleth styling based on energy statistics, and click event handling for coordinate selection\n",
    "- **Plotly Visualization Migration**: Guidance on converting matplotlib static plots to interactive Plotly visualizations across all pages, including line charts, bar charts, spectrograms, and polar wind rose charts\n",
    "- **SARIMAX Forecasting**: Assistance with implementing the statsmodels SARIMAX model, handling timezone alignment between weather and energy data, configuring model parameters, and creating confidence interval visualizations\n",
    "- **Sliding Window Correlation**: Help with implementing rolling correlation calculations with configurable lag and window size, and merging weather and energy time series data\n",
    "- **Snow Drift Calculations**: Guidance on adapting the snow drift calculation formulas, implementing sector-based wind transport analysis, and creating the polar wind rose visualization\n",
    "- **Error Handling**: Adding checks for missing data connections, handling NaN values, and providing user-friendly feedback messages throughout the application\n",
    "- **Code Documentation**: Writing detailed comments explaining each step, database operations, and data transformation logic\n",
    "\n",
    "AI tools significantly improved development efficiency, code quality, and implementation of complex statistical and machine learning methods while maintaining full understanding of the underlying algorithms and data pipelines."
   ]
  },
  {
   "cell_type": "markdown",
   "id": "2c6a1207",
   "metadata": {},
   "source": [
    "## Work Log\n",
    "\n",
    "In this fourth assignment, I expanded the project significantly by retrieving additional years of energy data, adding consumption data, implementing interactive maps with GeoJSON overlays, and building machine learning-based forecasting capabilities. The work involved substantial Streamlit app refactoring and the addition of several new analytical features.\n",
    "\n",
    "### Jupyter Notebook Development\n",
    "\n",
    "#### Extended Data Retrieval\n",
    "I began by retrieving hourly production data from the Elhub API for years 2022-2024, appending it to the existing 2021 data in both Cassandra and MongoDB. The same month-by-month fetching strategy from Part 2 was used to handle API limitations. Additionally, I retrieved hourly consumption data (`CONSUMPTION_PER_GROUP_MBA_HOUR`) for all price areas from 2021-2024. This required creating a new Cassandra table (`consumption_data`) with an appropriate schema using `consumptionGroup` instead of `productionGroup` as part of the composite partition key.\n",
    "\n",
    "### Streamlit App Refactoring\n",
    "\n",
    "#### Visualization Migration\n",
    "I converted all static matplotlib/seaborn plots to dynamic Plotly visualizations throughout the application. This included interactive line charts, bar charts with hover information, spectrograms with color scaling, polar wind rose charts, and time series forecasts with confidence intervals.\n",
    "\n",
    "#### Navigation Restructuring\n",
    "The page structure was reorganized into logical groupings: Data Exploration (Map Selector, Interactive Plot), Analysis (Snow Drift, Advanced Analysis, Weather Anomalies), and Predictive (Correlations, Forecasting). Color-coded CSS styling in the sidebar distinguishes between these groups, and the home page provides quick navigation buttons to each section.\n",
    "\n",
    "### New Streamlit Features\n",
    "\n",
    "#### Map Selector with GeoJSON Overlays\n",
    "I implemented a Folium map displaying Norwegian electricity price areas (NO1-NO5) using GeoJSON data downloaded from NVE. The map features: choropleth coloring based on mean production/consumption values for a user-selected time interval, click-to-select functionality that stores coordinates and identifies the corresponding price area, and dynamic layer switching between price areas (zoomed out) and municipalities (zoomed in). Selected locations are marked with a red pin and persist across page navigation.\n",
    "\n",
    "#### Snow Drift Analysis\n",
    "I adapted the provided Snow_drift.py calculations into a Streamlit page that computes annual and monthly snow transport based on wind speed data downloaded for the user-selected coordinates. The page includes configurable parameters (transport distance, fetch distance, relocation coefficient), season selection (July-June years), annual bar charts showing transport volumes with control type indicators, and a polar wind rose showing directional transport distribution.\n",
    "\n",
    "#### Sliding Window Correlation\n",
    "I transformed the correlation analysis from Part 3 into an interactive Streamlit page with selectors for weather variables (temperature, precipitation, wind speed, etc.) and energy data (production or consumption by area and group). Users can configure window size and lag parameters, and the page displays a rolling correlation time series along with a dual-axis plot showing both datasets.\n",
    "\n",
    "#### SARIMAX Forecasting\n",
    "I created a comprehensive forecasting interface allowing users to select energy data type, price area, and group, configure training date ranges and forecast horizons, set all SARIMAX parameters (p, d, q, P, D, Q, seasonal period, trend), and optionally include weather variables as exogenous features. The results display the forecast with 95% confidence intervals, validation metrics (MAE, RMSE) when test data is available, and proper timezone alignment between datasets.\n",
    "\n",
    "### Bonus Features\n",
    "\n",
    "I implemented several bonus tasks from the provided list:\n",
    "\n",
    "1. **Waiting time indicators**: Progress spinners are used throughout the app during data loading and model training. Users receive clear visual feedback when operations are in progress.\n",
    "\n",
    "2. **Error handling**: Data requirements are checked at page load with informative warnings directing users to complete prerequisites (e.g., selecting a location on the map before running snow drift analysis). Database connection errors and missing data are caught gracefully with user-friendly messages instead of cryptic error traces.\n",
    "\n",
    "3. **Map page with municipalities**: The map dynamically switches between price area polygons (when zoomed out) and municipality boundaries (when zoomed in beyond a natural threshold). Municipality GeoJSON data was downloaded from Geonorge using the EUREF 89 Geografisk (ETRS 89) 2d format. Each municipality is automatically mapped to its corresponding price area for consistent choropleth coloring.\n",
    "\n",
    "4. **Monthly snow drift**: In addition to annual snow transport calculations, the Snow Drift page now computes and displays monthly snow drift values. A dedicated \"Monthly Breakdown\" tab shows a line chart comparing monthly transport across selected seasons, allowing users to identify seasonal patterns in snow drift.\n",
    "\n",
    "5. **Weather as exogenous variables in forecasting**: The SARIMAX forecasting page allows users to select weather properties (temperature, precipitation, wind speed, wind gusts, wind direction) as exogenous variables. When selected, the app automatically downloads the required weather data for the training period and forecast horizon, properly aligning timezones and handling missing values through interpolation.\n",
    "\n",
    "### Correlation Analysis Findings\n",
    "\n",
    "While testing the sliding window correlation feature, I observed that wind speed shows stronger correlation with wind-based energy production during winter months when wind patterns are more consistent. Temperature correlations with consumption are strongest during heating season (October-March). Extreme weather events (storms) cause temporary spikes in correlation values, followed by decorrelation periods during recovery. These patterns suggest that adaptive forecasting models could benefit from dynamic feature weighting based on seasonal and weather conditions."
   ]
  },
  {
   "cell_type": "markdown",
   "id": "e3d37885",
   "metadata": {},
   "source": [
    "## 1. Setup and Connections\n",
    "Import necessary libraries and establish connections to MongoDB, Cassandra, and Spark."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "id": "4c8f6f8f",
   "metadata": {},
   "outputs": [],
   "source": [
    "import pandas as pd\n",
    "import requests\n",
    "from datetime import datetime, timedelta\n",
    "import pytz\n",
    "from pymongo.mongo_client import MongoClient\n",
    "from pymongo.server_api import ServerApi\n",
    "from dotenv import load_dotenv\n",
    "import os\n",
    "from cassandra.cluster import Cluster\n",
    "from pyspark.sql import SparkSession\n",
    "from pyspark.sql.functions import to_timestamp\n",
    "\n",
    "load_dotenv()\n",
    "\n",
    "# Set Hadoop home directory path (required for PySpark on Windows)\n",
    "os.environ[\"HADOOP_HOME\"] = \"C:/Hadoop/hadoop-3.3.1\"\n",
    "os.environ[\"PYSPARK_HADOOP_VERSION\"] = \"without\""
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "id": "07de8baf",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Successfully connected to MongoDB!\n"
     ]
    }
   ],
   "source": [
    "# MongoDB Connection\n",
    "uri = os.getenv(\"URI\")\n",
    "client = MongoClient(uri, server_api=ServerApi('1'))\n",
    "try:\n",
    "    client.admin.command('ping')\n",
    "    print(\"Successfully connected to MongoDB!\")\n",
    "    production_db = client['energy_data'] # Initialize database reference\n",
    "except Exception as e:\n",
    "    print(e)\n",
    "\n",
    "# Cassandra Connection\n",
    "cluster = Cluster(['localhost'], port=9042)\n",
    "session = cluster.connect()\n",
    "session.set_keyspace('my_first_keyspace')\n",
    "\n",
    "# Spark Session\n",
    "spark = SparkSession.builder.appName('SparkCassandraApp').\\\n",
    "    config('spark.jars.packages', 'com.datastax.spark:spark-cassandra-connector_2.12:3.5.1').\\\n",
    "    config('spark.cassandra.connection.host', 'localhost').\\\n",
    "    config('spark.sql.extensions', 'com.datastax.spark.connector.CassandraSparkExtensions').\\\n",
    "    config('spark.sql.catalog.mycatalog', 'com.datastax.spark.connector.datasource.CassandraCatalog').\\\n",
    "    config('spark.cassandra.connection.port', '9042').getOrCreate()"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "ee840528",
   "metadata": {},
   "source": [
    "## 2. Retrieve and Append Production Data (2022 - 2024)\n",
    "We will retrieve hourly production data for all price areas for the years 2022 through 2024 and append it to our existing datasets in Cassandra and MongoDB."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "id": "3d4b10b9",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Fetching production data from 2022-01-01 00:00:00+01:00 to 2024-12-31 23:59:59+01:00...\n",
      "Fetched 2022-01-01 to 2022-01-31\n",
      "Fetched 2022-02-01 to 2022-02-28\n",
      "Fetched 2022-03-01 to 2022-03-31\n",
      "Fetched 2022-04-01 to 2022-04-30\n",
      "Fetched 2022-05-01 to 2022-05-31\n",
      "Fetched 2022-06-01 to 2022-06-30\n",
      "Fetched 2022-07-01 to 2022-07-31\n",
      "Fetched 2022-08-01 to 2022-08-31\n",
      "Fetched 2022-09-01 to 2022-09-30\n",
      "Fetched 2022-10-01 to 2022-10-31\n",
      "Fetched 2022-11-01 to 2022-11-30\n",
      "Fetched 2022-12-01 to 2022-12-31\n",
      "Fetched 2023-01-01 to 2023-01-31\n",
      "Fetched 2023-02-01 to 2023-02-28\n",
      "Fetched 2023-03-01 to 2023-03-31\n",
      "Fetched 2023-04-01 to 2023-04-30\n",
      "Fetched 2023-05-01 to 2023-05-31\n",
      "Fetched 2023-06-01 to 2023-06-30\n",
      "Fetched 2023-07-01 to 2023-07-31\n",
      "Fetched 2023-08-01 to 2023-08-31\n",
      "Fetched 2023-09-01 to 2023-09-30\n",
      "Fetched 2023-10-01 to 2023-10-31\n",
      "Fetched 2023-11-01 to 2023-11-30\n",
      "Fetched 2023-12-01 to 2023-12-31\n",
      "Fetched 2024-01-01 to 2024-01-31\n",
      "Fetched 2024-02-01 to 2024-02-29\n",
      "Fetched 2024-03-01 to 2024-03-31\n",
      "Fetched 2024-04-01 to 2024-04-30\n",
      "Fetched 2024-05-01 to 2024-05-31\n",
      "Fetched 2024-06-01 to 2024-06-30\n",
      "Fetched 2024-07-01 to 2024-07-31\n",
      "Fetched 2024-08-01 to 2024-08-31\n",
      "Fetched 2024-09-01 to 2024-09-30\n",
      "Fetched 2024-10-01 to 2024-10-31\n",
      "Fetched 2024-11-01 to 2024-11-30\n",
      "Fetched 2024-12-01 to 2024-12-31\n"
     ]
    }
   ],
   "source": [
    "# API Configuration\n",
    "base_url = \"https://api.elhub.no/energy-data/v0/price-areas\"\n",
    "dataset = \"PRODUCTION_PER_GROUP_MBA_HOUR\"\n",
    "headers = {}\n",
    "\n",
    "# Timezone setup\n",
    "oslo_tz = pytz.timezone('Europe/Oslo')\n",
    "start_date = oslo_tz.localize(datetime(2022, 1, 1, 0, 0, 0))\n",
    "end_date = oslo_tz.localize(datetime(2024, 12, 31, 23, 59, 59))\n",
    "\n",
    "# Fetch data\n",
    "responses = []\n",
    "current_start = start_date\n",
    "\n",
    "print(f\"Fetching production data from {start_date} to {end_date}...\")\n",
    "\n",
    "while current_start <= end_date:\n",
    "    # Calculate month end\n",
    "    month_end = (current_start.replace(day=28) + timedelta(days=4)).replace(day=1) - timedelta(seconds=1)\n",
    "    if month_end > end_date:\n",
    "        month_end = end_date\n",
    "\n",
    "    params = {\n",
    "        \"dataset\": dataset,\n",
    "        \"startDate\": current_start.isoformat(),\n",
    "        \"endDate\": month_end.isoformat()\n",
    "    }\n",
    "\n",
    "    try:\n",
    "        response = requests.get(base_url, headers=headers, params=params)\n",
    "        response.raise_for_status()\n",
    "        responses.append(response.json())\n",
    "        print(f\"Fetched {current_start.date()} to {month_end.date()}\")\n",
    "    except requests.exceptions.RequestException as e:\n",
    "        print(f\"Error fetching data for {current_start.date()}: {e}\")\n",
    "\n",
    "    current_start = month_end + timedelta(seconds=1)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "id": "4beab4e0",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "New production records fetched: 657600\n",
      "                     endTime            lastUpdatedTime priceArea  \\\n",
      "0  2022-01-01T01:00:00+01:00  2025-02-01T18:02:57+01:00       NO1   \n",
      "1  2022-01-01T02:00:00+01:00  2025-02-01T18:02:57+01:00       NO1   \n",
      "2  2022-01-01T03:00:00+01:00  2025-02-01T18:02:57+01:00       NO1   \n",
      "3  2022-01-01T04:00:00+01:00  2025-02-01T18:02:57+01:00       NO1   \n",
      "4  2022-01-01T05:00:00+01:00  2025-02-01T18:02:57+01:00       NO1   \n",
      "\n",
      "  productionGroup  quantityKwh                  startTime  \n",
      "0           hydro    1291422.4  2022-01-01T00:00:00+01:00  \n",
      "1           hydro    1246209.4  2022-01-01T01:00:00+01:00  \n",
      "2           hydro    1271757.0  2022-01-01T02:00:00+01:00  \n",
      "3           hydro    1204251.8  2022-01-01T03:00:00+01:00  \n",
      "4           hydro    1202086.9  2022-01-01T04:00:00+01:00  \n"
     ]
    }
   ],
   "source": [
    "# Process Responses\n",
    "all_production_data = []\n",
    "\n",
    "for response in responses:\n",
    "    if 'data' in response:\n",
    "        for price_area in response['data']:\n",
    "            if 'attributes' in price_area and 'productionPerGroupMbaHour' in price_area['attributes']:\n",
    "                production_list = price_area['attributes']['productionPerGroupMbaHour']\n",
    "                if production_list:\n",
    "                    all_production_data.extend(production_list)\n",
    "\n",
    "df_production = pd.DataFrame(all_production_data)\n",
    "print(f\"New production records fetched: {len(df_production)}\")\n",
    "print(df_production.head())"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "a9bbeb20",
   "metadata": {},
   "source": [
    "### Append to Cassandra"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "id": "90de4545",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Production data (2022-2024) appended to Cassandra.\n"
     ]
    }
   ],
   "source": [
    "if not df_production.empty:\n",
    "    # Prepare Spark DataFrame\n",
    "    spark_df_prod = spark.createDataFrame(df_production)\n",
    "\n",
    "    # Rename columns\n",
    "    spark_df_prod = spark_df_prod.withColumnRenamed(\"priceArea\", \"price_area\") \\\n",
    "                       .withColumnRenamed(\"productionGroup\", \"production_group\") \\\n",
    "                       .withColumnRenamed(\"startTime\", \"start_time\") \\\n",
    "                       .withColumnRenamed(\"endTime\", \"end_time\") \\\n",
    "                       .withColumnRenamed(\"quantityKwh\", \"quantity_kwh\") \\\n",
    "                       .withColumnRenamed(\"lastUpdatedTime\", \"last_updated_time\")\n",
    "\n",
    "    # Convert timestamps\n",
    "    spark_df_prod = spark_df_prod.withColumn(\"start_time\", to_timestamp(\"start_time\")) \\\n",
    "                       .withColumn(\"end_time\", to_timestamp(\"end_time\")) \\\n",
    "                       .withColumn(\"last_updated_time\", to_timestamp(\"last_updated_time\"))\n",
    "\n",
    "    # Append to Cassandra\n",
    "    spark_df_prod.write \\\n",
    "        .format(\"org.apache.spark.sql.cassandra\") \\\n",
    "        .mode(\"append\") \\\n",
    "        .options(table=\"production_data\", keyspace=\"my_first_keyspace\") \\\n",
    "        .save()\n",
    "\n",
    "    print(\"Production data (2022-2024) appended to Cassandra.\")\n",
    "else:\n",
    "    print(\"No production data to append.\")"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "1fe420cb",
   "metadata": {},
   "source": [
    "### Append to MongoDB"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "id": "145be615",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Cleared existing data in 'production' collection.\n",
      "Appended 657600 documents to MongoDB 'production' collection.\n"
     ]
    }
   ],
   "source": [
    "if not df_production.empty:\n",
    "    production_collection = production_db['production']\n",
    "\n",
    "    # Clear existing data\n",
    "    production_collection.delete_many({})\n",
    "    print(\"Cleared existing data in 'production' collection.\")\n",
    "\n",
    "    production_records = df_production.to_dict('records')\n",
    "    \n",
    "    # Append data\n",
    "    result = production_collection.insert_many(production_records)\n",
    "    print(f\"Appended {len(result.inserted_ids)} documents to MongoDB 'production' collection.\")\n",
    "else:\n",
    "    print(\"No production data to append.\")"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "4d651826",
   "metadata": {},
   "source": [
    "## 3. Retrieve and Store Consumption Data (2021 - 2024)\n",
    "Now we will retrieve hourly consumption data (`CONSUMPTION_PER_GROUP_MBA_HOUR`) for the years 2021 through 2024."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "id": "bd71da5b",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Fetching consumption data from 2021-01-01 00:00:00+01:00 to 2024-12-31 23:59:59+01:00...\n",
      "Fetched 2021-01-01 to 2021-01-31\n",
      "Fetched 2021-02-01 to 2021-02-28\n",
      "Fetched 2021-03-01 to 2021-03-31\n",
      "Fetched 2021-04-01 to 2021-04-30\n",
      "Fetched 2021-05-01 to 2021-05-31\n",
      "Fetched 2021-06-01 to 2021-06-30\n",
      "Fetched 2021-07-01 to 2021-07-31\n",
      "Fetched 2021-08-01 to 2021-08-31\n",
      "Fetched 2021-09-01 to 2021-09-30\n",
      "Fetched 2021-10-01 to 2021-10-31\n",
      "Fetched 2021-11-01 to 2021-11-30\n",
      "Fetched 2021-12-01 to 2021-12-31\n",
      "Fetched 2022-01-01 to 2022-01-31\n",
      "Fetched 2022-02-01 to 2022-02-28\n",
      "Fetched 2022-03-01 to 2022-03-31\n",
      "Fetched 2022-04-01 to 2022-04-30\n",
      "Fetched 2022-05-01 to 2022-05-31\n",
      "Fetched 2022-06-01 to 2022-06-30\n",
      "Fetched 2022-07-01 to 2022-07-31\n",
      "Fetched 2022-08-01 to 2022-08-31\n",
      "Fetched 2022-09-01 to 2022-09-30\n",
      "Fetched 2022-10-01 to 2022-10-31\n",
      "Fetched 2022-11-01 to 2022-11-30\n",
      "Fetched 2022-12-01 to 2022-12-31\n",
      "Fetched 2023-01-01 to 2023-01-31\n",
      "Fetched 2023-02-01 to 2023-02-28\n",
      "Fetched 2023-03-01 to 2023-03-31\n",
      "Fetched 2023-04-01 to 2023-04-30\n",
      "Fetched 2023-05-01 to 2023-05-31\n",
      "Fetched 2023-06-01 to 2023-06-30\n",
      "Fetched 2023-07-01 to 2023-07-31\n",
      "Fetched 2023-08-01 to 2023-08-31\n",
      "Fetched 2023-09-01 to 2023-09-30\n",
      "Fetched 2023-10-01 to 2023-10-31\n",
      "Fetched 2023-11-01 to 2023-11-30\n",
      "Fetched 2023-12-01 to 2023-12-31\n",
      "Fetched 2024-01-01 to 2024-01-31\n",
      "Fetched 2024-02-01 to 2024-02-29\n",
      "Fetched 2024-03-01 to 2024-03-31\n",
      "Fetched 2024-04-01 to 2024-04-30\n",
      "Fetched 2024-05-01 to 2024-05-31\n",
      "Fetched 2024-06-01 to 2024-06-30\n",
      "Fetched 2024-07-01 to 2024-07-31\n",
      "Fetched 2024-08-01 to 2024-08-31\n",
      "Fetched 2024-09-01 to 2024-09-30\n",
      "Fetched 2024-10-01 to 2024-10-31\n",
      "Fetched 2024-11-01 to 2024-11-30\n",
      "Fetched 2024-12-01 to 2024-12-31\n"
     ]
    }
   ],
   "source": [
    "# API Configuration for Consumption\n",
    "dataset_consumption = \"CONSUMPTION_PER_GROUP_MBA_HOUR\"\n",
    "start_date_cons = oslo_tz.localize(datetime(2021, 1, 1, 0, 0, 0))\n",
    "end_date_cons = oslo_tz.localize(datetime(2024, 12, 31, 23, 59, 59))\n",
    "\n",
    "responses_cons = []\n",
    "current_start = start_date_cons\n",
    "\n",
    "print(f\"Fetching consumption data from {start_date_cons} to {end_date_cons}...\")\n",
    "\n",
    "while current_start <= end_date_cons:\n",
    "    month_end = (current_start.replace(day=28) + timedelta(days=4)).replace(day=1) - timedelta(seconds=1)\n",
    "    if month_end > end_date_cons:\n",
    "        month_end = end_date_cons\n",
    "\n",
    "    params = {\n",
    "        \"dataset\": dataset_consumption,\n",
    "        \"startDate\": current_start.isoformat(),\n",
    "        \"endDate\": month_end.isoformat()\n",
    "    }\n",
    "\n",
    "    try:\n",
    "        response = requests.get(base_url, headers=headers, params=params)\n",
    "        response.raise_for_status()\n",
    "        responses_cons.append(response.json())\n",
    "        print(f\"Fetched {current_start.date()} to {month_end.date()}\")\n",
    "    except requests.exceptions.RequestException as e:\n",
    "        print(f\"Error fetching data for {current_start.date()}: {e}\")\n",
    "\n",
    "    current_start = month_end + timedelta(seconds=1)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "id": "7def0c0c",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Total consumption records fetched: 876600\n",
      "  consumptionGroup                    endTime            lastUpdatedTime  \\\n",
      "0            cabin  2021-01-01T01:00:00+01:00  2024-12-20T10:35:40+01:00   \n",
      "1            cabin  2021-01-01T02:00:00+01:00  2024-12-20T10:35:40+01:00   \n",
      "2            cabin  2021-01-01T03:00:00+01:00  2024-12-20T10:35:40+01:00   \n",
      "3            cabin  2021-01-01T04:00:00+01:00  2024-12-20T10:35:40+01:00   \n",
      "4            cabin  2021-01-01T05:00:00+01:00  2024-12-20T10:35:40+01:00   \n",
      "\n",
      "   meteringPointCount priceArea  quantityKwh                  startTime  \n",
      "0              100607       NO1    177071.56  2021-01-01T00:00:00+01:00  \n",
      "1              100607       NO1    171335.12  2021-01-01T01:00:00+01:00  \n",
      "2              100607       NO1    164912.02  2021-01-01T02:00:00+01:00  \n",
      "3              100607       NO1    160265.77  2021-01-01T03:00:00+01:00  \n",
      "4              100607       NO1    159828.69  2021-01-01T04:00:00+01:00  \n"
     ]
    }
   ],
   "source": [
    "# Process Consumption Responses\n",
    "all_consumption_data = []\n",
    "\n",
    "for response in responses_cons:\n",
    "    if 'data' in response:\n",
    "        for price_area in response['data']:\n",
    "            if 'attributes' in price_area and 'consumptionPerGroupMbaHour' in price_area['attributes']:\n",
    "                consumption_list = price_area['attributes']['consumptionPerGroupMbaHour']\n",
    "                if consumption_list:\n",
    "                    all_consumption_data.extend(consumption_list)\n",
    "\n",
    "df_consumption = pd.DataFrame(all_consumption_data)\n",
    "print(f\"Total consumption records fetched: {len(df_consumption)}\")\n",
    "print(df_consumption.head())"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "5db51b87",
   "metadata": {},
   "source": [
    "### Store in Cassandra\n",
    "We will create a new table `consumption_data` for this dataset."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "id": "812aeb27",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Table 'consumption_data' created.\n",
      "Consumption data inserted into Cassandra.\n"
     ]
    }
   ],
   "source": [
    "# Create consumption table\n",
    "# Note: The API returns 'consumptionGroup' instead of 'productionGroup'\n",
    "session.execute(\"DROP TABLE IF EXISTS my_first_keyspace.consumption_data;\")\n",
    "\n",
    "create_cons_table_query = \"\"\"\n",
    "CREATE TABLE IF NOT EXISTS my_first_keyspace.consumption_data (\n",
    "    price_area text,\n",
    "    consumption_group text,\n",
    "    start_time timestamp,\n",
    "    end_time timestamp,\n",
    "    quantity_kwh double,\n",
    "    last_updated_time timestamp,\n",
    "    PRIMARY KEY ((price_area, consumption_group), start_time)\n",
    ") WITH CLUSTERING ORDER BY (start_time ASC);\n",
    "\"\"\"\n",
    "session.execute(create_cons_table_query)\n",
    "print(\"Table 'consumption_data' created.\")\n",
    "\n",
    "if not df_consumption.empty:\n",
    "    spark_df_cons = spark.createDataFrame(df_consumption)\n",
    "\n",
    "    # Rename columns (consumptionGroup -> consumption_group)\n",
    "    spark_df_cons = (\n",
    "        spark_df_cons\n",
    "        .withColumnRenamed(\"priceArea\", \"price_area\")\n",
    "        .withColumnRenamed(\"consumptionGroup\", \"consumption_group\")\n",
    "        .withColumnRenamed(\"startTime\", \"start_time\")\n",
    "        .withColumnRenamed(\"endTime\", \"end_time\")\n",
    "        .withColumnRenamed(\"quantityKwh\", \"quantity_kwh\")\n",
    "        .withColumnRenamed(\"lastUpdatedTime\", \"last_updated_time\")\n",
    "    )\n",
    "\n",
    "    # Drop columns that are not in the Cassandra table schema\n",
    "    spark_df_cons = spark_df_cons.select(\n",
    "        \"price_area\",\n",
    "        \"consumption_group\",\n",
    "        \"start_time\",\n",
    "        \"end_time\",\n",
    "        \"quantity_kwh\",\n",
    "        \"last_updated_time\",\n",
    "    )\n",
    "\n",
    "    # Convert timestamps\n",
    "    spark_df_cons = (\n",
    "        spark_df_cons\n",
    "        .withColumn(\"start_time\", to_timestamp(\"start_time\"))\n",
    "        .withColumn(\"end_time\", to_timestamp(\"end_time\"))\n",
    "        .withColumn(\"last_updated_time\", to_timestamp(\"last_updated_time\"))\n",
    "    )\n",
    "\n",
    "    spark_df_cons.write \\\n",
    "        .format(\"org.apache.spark.sql.cassandra\") \\\n",
    "        .mode(\"append\") \\\n",
    "        .options(table=\"consumption_data\", keyspace=\"my_first_keyspace\") \\\n",
    "        .save()\n",
    "\n",
    "    print(\"Consumption data inserted into Cassandra.\")\n",
    "else:\n",
    "    print(\"No consumption data to write.\")"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "fb553cea",
   "metadata": {},
   "source": [
    "### Store in MongoDB\n",
    "We will use a new collection `consumption` and clear any existing data before inserting."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "id": "2752f95e",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Cleared existing data in 'consumption' collection.\n",
      "Inserted 876600 documents into MongoDB 'consumption' collection.\n"
     ]
    }
   ],
   "source": [
    "if not df_consumption.empty:\n",
    "    consumption_collection = production_db['consumption']\n",
    "\n",
    "    # Clear existing data\n",
    "    consumption_collection.delete_many({})\n",
    "    print(\"Cleared existing data in 'consumption' collection.\")\n",
    "\n",
    "    consumption_records = df_consumption.to_dict('records')\n",
    "    \n",
    "    # Insert new data\n",
    "    result = consumption_collection.insert_many(consumption_records)\n",
    "    print(f\"Inserted {len(result.inserted_ids)} documents into MongoDB 'consumption' collection.\")"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": ".venv (3.11.9)",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.11.9"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
